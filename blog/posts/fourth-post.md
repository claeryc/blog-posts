---
title: AI is becoming inbred
thumbnail: /static/images/ghostp.jpg
date: 2025-08-15
---
# AI is becoming inbred
My prediction of changes in **artificial intelligence**.

If an AI image generator (LLM) is repeatedly fed the same information,  
or if users repeatedly feed its own outputs back as input,  
it can begin to amplify errors or hallucinations already present in the model.  
This negative outcome can appear after just a few iterations,  
leading to images that diverge significantly from the original—  
a process sometimes called "inbreeding," a form of degenerative learning.

Importantly, the “truth” of what AI outputs is not objective:  
it depends heavily on the data provided by users.  
The more AI-generated content circulates online,  
and the more user prompts reflect subjective interpretations or errors,  
the more future models trained on that data may reproduce those biases or distortions.  
In other words, user behavior shapes not just individual outputs,  
but the model itself over time.

The procedure can play out as follows:  
A user prompts an AI model (e.g., ChatGPT) to create an image:  

**Text input:** "create an image of Mona Lisa"  
- The model generates an image of the Mona Lisa.  

**Text input:** "describe the image"  
- The model produces a description that diverges slightly from the original.  

**Text input:** "create an image which looks like this:"  
- The user inserts the generated description into a new prompt.  
- The model generates a new image, which now differs from the original version.  

![](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41586-024-07566-y/MediaObjects/41586_2024_7566_Fig1_HTML.png?as=webp)

If this process is repeated enough times, the model can eventually produce something entirely different (in this example, a god surrounded by galaxies).  
It is speculated that if a large number of AI-generated images are uploaded to the internet,  
models may gradually produce lower-quality images—and text, among other outputs—over time.

This phenomenon has historical parallels with earlier AI systems.  
For example, Microsoft’s chatbot on X (formerly Twitter), named [Tay.ai](https://en.wikipedia.org/wiki/Tay(chatbot)),  
quickly began producing problematic outputs due to learning from unfiltered user interactions,  
for which Microsoft issued an open [apology letter](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/).  
On a side note, this wasn’t Microsoft’s first AI chatbot.  
The first one appeared in China, called [XiaoIce](https://en.wikipedia.org/wiki/Xiaoice),  
which reached over 40 million users ([source](https://news.microsoft.com/apac/features/much-more-than-a-chatbot-chinas-xiaoice-mixes-ai-with-emotions-and-wins-over-millions-of-fans/)).